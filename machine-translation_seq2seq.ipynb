{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50  \n",
    "LATENT_DIM = 256\n",
    "NUM_SAMPLES = 10000  \n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fun.\n",
      "যাও। <eos>\n",
      "<sos> যাও।\n",
      "4349\n"
     ]
    }
   ],
   "source": [
    "input_texts=[]\n",
    "target_texts=[]\n",
    "target_texts_inputs=[]\n",
    "t=0\n",
    "for line in open('ben.txt',encoding=\"utf-8\"):\n",
    "    t+=1\n",
    "    if t>NUM_SAMPLES:\n",
    "        break\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    # split up the input and translation\n",
    "    input_text,translation,_=line.rstrip().split('\\t')\n",
    "    target_text=translation + ' <eos>'\n",
    "    target_text_input='<sos> ' + translation\n",
    "    \n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)\n",
    "\n",
    "    \n",
    "print(input_texts[152])\n",
    "print(target_texts[0])\n",
    "print(target_texts_inputs[0])\n",
    "print(t)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 421]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_input=Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_input.fit_on_texts(input_texts)\n",
    "input_sequences=tokenizer_input.texts_to_sequences(input_texts)\n",
    "print(input_sequences[152])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[31, 243, 53, 32, 88]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_input.texts_to_sequences([\"She knows where we live\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1875 unique input tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx_inputs=tokenizer_input.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
    "word2idx_inputs['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "max_len_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[167, 1]\n",
      "[2, 167]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_output=Tokenizer(num_words=MAX_NUM_WORDS,filters='')\n",
    "tokenizer_output.fit_on_texts(target_texts+target_texts_inputs)\n",
    "target_sequences=tokenizer_output.texts_to_sequences(target_texts)\n",
    "target_inputs_sequences=tokenizer_output.texts_to_sequences(target_texts_inputs)\n",
    "print(target_sequences[0])\n",
    "print(target_inputs_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3551 unique output tokens.\n"
     ]
    }
   ],
   "source": [
    "word2idx_output = tokenizer_output.word_index\n",
    "print('Found %s unique output tokens.' % len(word2idx_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words_output = len(word2idx_output) + 1\n",
    "max_len_target = max(len(s) for s in target_sequences)\n",
    "max_len_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sequnces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (4349, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs=pad_sequences(input_sequences,max_len_input)\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "encoder_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_inputs.shape: (4349, 19)\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = pad_sequences(target_inputs_sequences, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec={}\n",
    "with open(r'D:\\udemy\\glove.6B.100d.txt',encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vec=np.asarray(values[1:],dtype='float32')\n",
    "        word2vec[word]=vec\n",
    "print('Found %s word vectors.' % len(word2vec))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "  if i < MAX_NUM_WORDS:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creation of  embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(num_words,EMBEDDING_DIM,weights=[embedding_matrix],input_length=max_len_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")\n",
    "\n",
    "# assign the values\n",
    "for i, d in enumerate(decoder_targets):\n",
    "  for t, word in enumerate(d):\n",
    "    if word != 0:\n",
    "      decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot[0,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ARKA4\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "encoder_input=Input(shape=(max_len_input,))\n",
    "x=embedding_layer(encoder_input)\n",
    "encoder=LSTM(LATENT_DIM,return_state=True,dropout=0.5)\n",
    "encoder_outputs,h,c=encoder(x)\n",
    "encoder_state=[h,c]\n",
    "decoder_input=Input(shape=(max_len_target,))\n",
    "decoder_embedding=Embedding(num_words_output,EMBEDDING_DIM)\n",
    "decoder_input_x=decoder_embedding(decoder_input)\n",
    "decoder=LSTM(LATENT_DIM,return_sequences=True,return_state=True,dropout=0.5)\n",
    "decoder_outputs,_,_=decoder(decoder_input_x,initial_state=encoder_state)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model=Model([encoder_input,decoder_input],decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 19)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 19)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 19, 100)      187600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 19, 100)      355200      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 365568      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 19, 256), (N 365568      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 19, 3552)     912864      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,186,800\n",
      "Trainable params: 2,186,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  mask = K.cast(y_true > 0, dtype='float32')\n",
    "  out = mask * y_true * K.log(y_pred)\n",
    "  return -K.sum(out) / K.sum(mask)\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  targ = K.argmax(y_true, axis=-1)\n",
    "  pred = K.argmax(y_pred, axis=-1)\n",
    "  correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "  # 0 is padding, don't include those\n",
    "  mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
    "  n_correct = K.sum(mask * correct)\n",
    "  n_total = K.sum(mask)\n",
    "  return n_correct / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ARKA4\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ARKA4\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/50\n",
      "4349/4349 [==============================] - 28s 6ms/step - loss: 6.2799 - acc: 0.2065\n",
      "Epoch 2/50\n",
      "4349/4349 [==============================] - 27s 6ms/step - loss: 5.4947 - acc: 0.2289\n",
      "Epoch 3/50\n",
      "4349/4349 [==============================] - 28s 6ms/step - loss: 5.2259 - acc: 0.2346\n",
      "Epoch 4/50\n",
      "4349/4349 [==============================] - 28s 6ms/step - loss: 5.0215 - acc: 0.2378\n",
      "Epoch 5/50\n",
      "4349/4349 [==============================] - 28s 6ms/step - loss: 4.8353 - acc: 0.2467\n",
      "Epoch 6/50\n",
      "4349/4349 [==============================] - 27s 6ms/step - loss: 4.6650 - acc: 0.2581\n",
      "Epoch 7/50\n",
      "4349/4349 [==============================] - 28s 6ms/step - loss: 4.4953 - acc: 0.2679\n",
      "Epoch 8/50\n",
      "4349/4349 [==============================] - 28s 6ms/step - loss: 4.3254 - acc: 0.2821\n",
      "Epoch 9/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 4.1559 - acc: 0.2980\n",
      "Epoch 10/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 3.9789 - acc: 0.3185\n",
      "Epoch 11/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 3.8149 - acc: 0.3368\n",
      "Epoch 12/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 3.6583 - acc: 0.3524\n",
      "Epoch 13/50\n",
      "4349/4349 [==============================] - 31s 7ms/step - loss: 3.5091 - acc: 0.3676\n",
      "Epoch 14/50\n",
      "4349/4349 [==============================] - 32s 7ms/step - loss: 3.3605 - acc: 0.3836\n",
      "Epoch 15/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 3.2252 - acc: 0.3973\n",
      "Epoch 16/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 3.0892 - acc: 0.4132\n",
      "Epoch 17/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 2.9597 - acc: 0.4273\n",
      "Epoch 18/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 2.8346 - acc: 0.4428\n",
      "Epoch 19/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 2.7185 - acc: 0.4567\n",
      "Epoch 20/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 2.6060 - acc: 0.4676\n",
      "Epoch 21/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 2.4976 - acc: 0.4868\n",
      "Epoch 22/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 2.3965 - acc: 0.5000\n",
      "Epoch 23/50\n",
      "4349/4349 [==============================] - 30s 7ms/step - loss: 2.2940 - acc: 0.5129\n",
      "Epoch 24/50\n",
      "4349/4349 [==============================] - 30s 7ms/step - loss: 2.2032 - acc: 0.5294\n",
      "Epoch 25/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 2.1096 - acc: 0.5453\n",
      "Epoch 26/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 2.0219 - acc: 0.5623\n",
      "Epoch 27/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.9435 - acc: 0.5768\n",
      "Epoch 28/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.8677 - acc: 0.5911\n",
      "Epoch 29/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.7968 - acc: 0.6063\n",
      "Epoch 30/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.7191 - acc: 0.6211\n",
      "Epoch 31/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.6535 - acc: 0.6311\n",
      "Epoch 32/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.5864 - acc: 0.6469\n",
      "Epoch 33/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.5300 - acc: 0.6568\n",
      "Epoch 34/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.4687 - acc: 0.6681\n",
      "Epoch 35/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.4196 - acc: 0.6765\n",
      "Epoch 36/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.3708 - acc: 0.6869\n",
      "Epoch 37/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.3192 - acc: 0.6962\n",
      "Epoch 38/50\n",
      "4349/4349 [==============================] - 31s 7ms/step - loss: 1.2682 - acc: 0.7075\n",
      "Epoch 39/50\n",
      "4349/4349 [==============================] - 30s 7ms/step - loss: 1.2270 - acc: 0.7153\n",
      "Epoch 40/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.1885 - acc: 0.7187\n",
      "Epoch 41/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.1436 - acc: 0.7293\n",
      "Epoch 42/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.1076 - acc: 0.7362\n",
      "Epoch 43/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 1.0699 - acc: 0.7459\n",
      "Epoch 44/50\n",
      "4349/4349 [==============================] - 28s 7ms/step - loss: 1.0314 - acc: 0.7522\n",
      "Epoch 45/50\n",
      "4349/4349 [==============================] - 27s 6ms/step - loss: 0.9962 - acc: 0.7578\n",
      "Epoch 46/50\n",
      "4349/4349 [==============================] - 27s 6ms/step - loss: 0.9727 - acc: 0.7607\n",
      "Epoch 47/50\n",
      "4349/4349 [==============================] - 27s 6ms/step - loss: 0.9383 - acc: 0.7693\n",
      "Epoch 48/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 0.9060 - acc: 0.7763\n",
      "Epoch 49/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 0.8785 - acc: 0.7852\n",
      "Epoch 50/50\n",
      "4349/4349 [==============================] - 29s 7ms/step - loss: 0.8515 - acc: 0.7866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x169ab9b5710>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_inputs, decoder_inputs], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model=Model(encoder_input,encoder_state)\n",
    "\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs,h,c=decoder(decoder_inputs_single_x,initial_state=decoder_states_inputs)\n",
    "decoder_states=[h,c]\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "  [decoder_inputs_single] + decoder_states_inputs, \n",
    "  [decoder_outputs] + decoder_states\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value=encoder_model.predict(input_seq)\n",
    "    target_seq=np.zeros((1,1))\n",
    "    target_seq[0,0]=word2idx_output['<sos>']\n",
    "    eos = word2idx_output['<eos>']\n",
    "    \n",
    "    output_sentence=[]\n",
    "    \n",
    "    for _ in range (max_len_target):\n",
    "        output_tokens,h,c=decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "        if idx==eos:\n",
    "            break\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value=[h,c]\n",
    "    return ' '.join(output_sentence)\n",
    "        \n",
    "            \n",
    "          \n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: I found that book interesting.\n",
      "Translation: আমার বইটা আকর্ষণীয় বলে মনে হয়েছিল।\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_texts))\n",
    "input_seq = encoder_inputs[i:i+1]\n",
    "translation = decode_sequence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_texts[i])\n",
    "print('Translation:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation(input):\n",
    "    sentence=[input]\n",
    "    sequence=tokenizer_input.texts_to_sequences(sentence)\n",
    "    input_seq=pad_sequences(sequence,max_len_input)\n",
    "    print(decode_sequence(input_seq))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আমি জাপানে থাকি।\n"
     ]
    }
   ],
   "source": [
    "sentence=[\"I wor\"]\n",
    "sequence=tokenizer_input.texts_to_sequences(sentence)\n",
    "input_seq=pad_sequences(sequence,max_len_input)\n",
    "print(decode_sequence(input_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
